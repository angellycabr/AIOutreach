# -*- coding: utf-8 -*-
"""Google AI Community - Fashion GAN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rkLqCTV5q0MkZdLLgYyWbP0eTyPhZOnE

### In this lesson, we'll design a Generative Adversarial Network (GAN) to generate pictures of clothes!
But first, we'll discuss the components that make up GANs


---



## Overview of Discriminative vs. Generative AI
## The difference between Discriminative and Generative AI
*   Discriminatory models learn to distinguish between classes based on given features in the data
    *   E.g. models trained to distinguish between dogs and cats
*   Generative AI models learn patterns in data to replicate them
    *   If given sufficent examples of dogs, can learn to create new images of them!
"""

# Import all necessary libraries. We won't go into detail about what they do.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras import layers, Sequential
from keras.datasets import fashion_mnist
from keras.optimizers import Adam

# Use keras to download the fashion MNIST dataset directly. We'll use this dataset to train our GAN.
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Visualize 25 apparel from the train dataset.
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

plt.figure(figsize = [10,12])
for i in range(25):
  plt.subplot(5,5,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(X_train[i], cmap='gray')
  plt.xlabel(class_names[y_train[i]])

# Flatten and scale the train and test images.
X_train = X_train.reshape(-1, 784)
X_train = (X_train / 255 - 0.5) * 2
X_test  = X_test.reshape(-1, 784)
X_test  = (X_test / 255 - 0.5) * 2

"""# What are Generative Adversarial Networks (GANs)?
GANs are a type of Neural Network used for generative AI. They are called Adversarial Networks because two Neural Networks compete against each other.
"""

import keras
import keras.backend as K
from keras.layers import Input, Dense, Activation, LeakyReLU, BatchNormalization
from keras.models import Sequential
from keras.optimizers import Adam

# Build the generator and discriminator networks.
def make_simple_GAN(sample_size):
    K.clear_session()

    # Generator network
    generator = Sequential()
    generator.add(Input(shape=(sample_size,)))
    generator.add(Dense(128, activation = LeakyReLU(alpha=0.01)))
    generator.add(Dense(256, activation = LeakyReLU(alpha=0.01)))
    generator.add(layers.BatchNormalization())
    generator.add(Dense(784, activation ='tanh'))

    # Discriminator network
    discriminator = Sequential()
    discriminator.add(Input(shape=(784,)))
    discriminator.add(Dense(128, activation = LeakyReLU(alpha=0.01)))
    discriminator.add(Dense(256, activation = LeakyReLU(alpha=0.01)))
    discriminator.add(Dense(1, activation ='sigmoid'))

    # Build the GAN network.
    gan = Sequential([generator, discriminator])

    discriminator.compile(optimizer = Adam(learning_rate=0.001), loss = 'binary_crossentropy')
    gan.compile(optimizer = Adam(learning_rate=0.0001), loss = 'binary_crossentropy')

    return gan, generator, discriminator

# Helper functions to generate noisy samples
def make_latent_samples(n_samples, sample_size):
    return np.random.normal(loc=0, scale=1, size=(n_samples, sample_size))

def make_trainable(model, trainable):
    for layer in model.layers:
        layer.trainable = trainable

"""# How do GANS learn?
The generative model gets feedback from the discriminative model. The goal of the generative model is to confuse the discriminative model until it reaches equilibrium (it's accuracy is 50%)!
"""

# Choose appropriate hyperparameters.
sample_size     = 100
epochs          = 50
batch_size      = 64
eval_size       = 16
smooth          = 0.1

# Create labels for real and fake images
y_train_real, y_train_fake = np.ones([batch_size, 1]), np.zeros([batch_size, 1])
y_eval_real,  y_eval_fake  = np.ones([eval_size, 1]), np.zeros([eval_size, 1])

gan, generator, discriminator = make_simple_GAN(sample_size)

losses = []
for e in range(epochs):
    if e % 10 == 0:
          latent_samples = make_latent_samples(25, sample_size)
          generated_digits = generator.predict(latent_samples)

          plt.figure(figsize=(5, 5))
          for i in range(25):
              img = generated_digits[i]
              img = (img / 2 + 0.5) * 255
              img = img.reshape(28, 28)
              plt.subplot(5, 5, i + 1)
              plt.imshow(img, cmap='gray')
              plt.axis('off')
          plt.show()

    for i in range(len(X_train)//batch_size):
        # Indices for real images
        X_batch_real = X_train[i*batch_size:(i+1)*batch_size]

        # Generate fake images
        latent_samples = make_latent_samples(batch_size, sample_size)
        X_batch_fake = generator.predict_on_batch(latent_samples)

        # Train discriminator
        make_trainable(discriminator, True)
        discriminator.train_on_batch(X_batch_real, y_train_real * (1 - smooth))
        discriminator.train_on_batch(X_batch_fake, y_train_fake)

        # Train generator
        make_trainable(discriminator, False)
        gan.train_on_batch(latent_samples, y_train_real)

    # Evaluate the model
    X_eval_real = X_test[np.random.choice(len(X_test), eval_size, replace=False)]

    latent_samples = make_latent_samples(eval_size, sample_size)
    X_eval_fake = generator.predict_on_batch(latent_samples)

    d_loss = discriminator.test_on_batch(X_eval_real, y_eval_real)
    d_loss = d_loss + discriminator.test_on_batch(X_eval_fake, y_eval_fake)

    # The fake images are labeled a real to test if the discriminator can be fooled.
    g_loss = gan.test_on_batch(latent_samples, y_eval_real)

    losses.append((d_loss, g_loss))

    print("Epoch: {:>3}/{} Discriminator Loss: {:>6.4f} Generator Loss: {:>6.4f}".format(
        e+1, epochs, d_loss, g_loss))

# Plot the loss curves for the discriminator and generator.
loss = np.array(losses)
plt.plot(loss[:, 0], label='Discriminator Loss')
plt.plot(loss[:, 1], label='Generator Loss')
plt.title('Training Losses')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# After the GAN is trained, generate and visualize 64 apparels.
latent_samples = make_latent_samples(64, sample_size)
generated_digits = generator.predict(latent_samples)

plt.figure(figsize=(5, 5))

for i in range(64):
  img = generated_digits[i]
  img = (img / 2 + 1)* 255
  img = img.reshape(28, 28)
  plt.subplot(8, 8, i+1)
  plt.imshow(img, cmap='gray')
  plt.xticks([])
  plt.yticks([])